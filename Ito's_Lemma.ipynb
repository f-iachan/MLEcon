{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/f-iachan/MLEcon/blob/master/Ito's_Lemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h7MhqC7ns0e"
      },
      "source": [
        "In this notebook I illustrate one of the key ingredients of the paper: How to compute expectations (in contiuous time) with almost no extra computional cost, regardless of the size of the state space.\n",
        "\n",
        "Consider the following example: There are 10000 state variables $x_i$ that follow the SDEs:\n",
        "\n",
        "$$\n",
        "    dx_i = \\mu_i(x) dt + \\sigma_i(x)^T dZ\n",
        "$$\n",
        "\n",
        "Where $dZ$ is a 100-dimensional brownian motion.\n",
        "\n",
        "\n",
        "Given an arbitrary function $f$, I will illustrate how to compute $\\mathbb{E}[\\frac{df}{dt}$]. I will use a neural network as the arbitrary function, but you could really use anything you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LteZA7gXno15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c3730b4-ca2d-4c62-be4b-5860a7944371"
      },
      "source": [
        "import jax\n",
        "from jax import jvp, grad, jit, vmap\n",
        "import numpy as onp\n",
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as np\n",
        "from jax.experimental import stax\n",
        "from jax.experimental.stax import Dense, Relu, Tanh, Sigmoid\n",
        "from functools import partial\n",
        "from jax.experimental import optimizers\n",
        "\n",
        "# Problem dimensions\n",
        "n_shocks = 100\n",
        "n_states = 10000\n",
        "\n",
        "# Random numbers seed\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "# Create neural network to represent the fu f\n",
        "initializer, f = stax.serial(\n",
        "    Dense(128), Tanh,\n",
        "    Dense(64), Tanh,\n",
        "    Dense(1))\n",
        "_, Θ = initializer(rng, (-1, n_states))\n",
        "\n",
        "\n",
        "# Setup the dynamics of the problem\n",
        "# postulate whatever dynamics you want\n",
        "def dynamics(x):\n",
        "    μ = -0.05 * x\n",
        "    σ = np.array([x] * n_shocks).T\n",
        "\n",
        "    return μ, σ\n",
        "\n",
        "\n",
        "# This is the heart of the paper: a generic function to compute the\n",
        "# drift of arbitrary functions with arbitrary numbers of state vars and\n",
        "# brownian shocks\n",
        "def drift(f, Θ, state, μstate, σstate):\n",
        "    f_flat = lambda state: np.squeeze(f(Θ, state))\n",
        "    first_order = jvp(f_flat, (state,), (μstate, ))[1]\n",
        "\n",
        "    def hvp(f, x, σ):\n",
        "        return jvp(grad(f), (x, ), (σ, ))[1]\n",
        "\n",
        "    second_order = np.sum(\n",
        "        np.array([hvp(f_flat, state, σstate.T[i]) @ σstate.T[i] for i\n",
        "         in range(n_shocks)]))\n",
        "\n",
        "    EdV = first_order + 0.5 * second_order\n",
        "    return EdV\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/stax.py:30: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/experimental/optimizers.py:30: FutureWarning: jax.experimental.optimizers is deprecated, import jax.example_libraries.optimizers instead\n",
            "  FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVUFvQispmvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3a72fa-606d-4b69-cf5b-5f4faabba3ac"
      },
      "source": [
        "# Let's get a sense of how costly it is to evaluate the original function, f,\n",
        "# for 512 different points picked at random\n",
        "\n",
        "x = onp.random.normal(size=[512, n_states])\n",
        "\n",
        "@jit\n",
        "def compute_f(x):\n",
        "    return f(Θ, x)\n",
        "\n",
        "compute_f(x)  # run it once to jit compile it\n",
        "%timeit compute_f(x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 9.83 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzwszn9ipg_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c85342-bfeb-46e0-8d35-549ea77e40bf"
      },
      "source": [
        "# Now let's see how long it takes to compute it's drift\n",
        "\n",
        "@vmap\n",
        "@jit\n",
        "def compute_Edf(x):\n",
        "    # Dynamics\n",
        "    μ, σ = dynamics(x)\n",
        "\n",
        "    # Ito's Lemma\n",
        "    EdV = drift(f, Θ, x, μ, σ)\n",
        "\n",
        "    return EdV\n",
        "\n",
        "compute_Edf(x)  # run it once to jit compile it\n",
        "%timeit compute_Edf(x).block_until_ready()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 58.2 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm27E8bKqieJ"
      },
      "source": [
        "# Conclusion:\n",
        "Computing the exact expectation took 60 ms, compared to 10ms that takes to compute the original function. Notice that we didn't have to compute a single partial derivative, either by hand or numerically. Let alone large and nasty Hessian matrices..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH9Ed3LXqj7-"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}